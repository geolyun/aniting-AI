import torch
import joblib
from transformers import BertTokenizer, BertModel

# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
model = joblib.load("rf_model.pkl")
tokenizer = BertTokenizer.from_pretrained("monologg/kobert")
bert_model = BertModel.from_pretrained("monologg/kobert")
bert_model.eval()

expected_traits = ["activity", "sociability", "care", "emotional_bond", "environment", "routine"]

# ì—¬ëŸ¬ ë¬¸ì¥ì˜ í‰ê·  ì„ë² ë”© ê³„ì‚°
def get_mean_embedding(texts: list[str]) -> torch.Tensor:
    embs = []
    for text in texts:
        inputs = tokenizer(text, return_tensors="pt", truncation=True, padding="max_length", max_length=128)
        with torch.no_grad():
            output = bert_model(**inputs)
        embs.append(output.pooler_output.squeeze(0))
    return torch.stack(embs).mean(dim=0).numpy().reshape(1, -1)

# ì„±í–¥ ì ìˆ˜ ì˜ˆì¸¡ í•¨ìˆ˜
def predict_traits(qna_list: list[str]) -> dict:
    emb = get_mean_embedding(qna_list)
    prediction = model.predict(emb)[0]
    result = {
        trait: min(5, max(1, int(round(score))))
        for trait, score in zip(expected_traits, prediction)
    }
    return result

# í„°ë¯¸ë„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ìš©
if __name__ == "__main__":
    print("10ê°œì˜ ì§ˆë¬¸+ì‘ë‹µì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: activity: ì–´ë–¤ í™œë™ì„ ì¢‹ì•„í•˜ì„¸ìš”? [SEP] ì‚°ì±…ì„ ìì£¼ í•©ë‹ˆë‹¤.)")

    qna_list = []
    for i in range(10):
        qna = input(f"{i+1} >> ")
        qna_list.append(qna)

    predictions = predict_traits(qna_list)

    print("\nğŸ“Š ì˜ˆì¸¡ëœ ì„±í–¥ ì ìˆ˜:")
    for trait, score in predictions.items():
        print(f"{trait}: {score}")