import torch
import joblib
from transformers import BertTokenizer, BertModel

# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
model = joblib.load("rf_model.pkl")
tokenizer = BertTokenizer.from_pretrained("monologg/kobert")
bert_model = BertModel.from_pretrained("monologg/kobert")
bert_model.eval()

expected_traits = ["activity", "sociability", "care", "emotional_bond", "environment", "routine"]

def get_embedding(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding="max_length", max_length=128)
    with torch.no_grad():
        outputs = bert_model(**inputs)
    return outputs.pooler_output.squeeze(0).numpy().reshape(1, -1)

def predict_traits(input_text: str) -> dict:
    emb = get_embedding(input_text)
    prediction = model.predict(emb)[0]
    result = {
        trait: min(5, max(1, int(round(score))))
        for trait, score in zip(expected_traits, prediction)
    }
    return result

if __name__ == "__main__":
    print("ì„±í–¥ ë¶„ì„ì„ ìœ„í•œ ë¬¸ì¥ì„ ì…ë ¥í•˜ì„¸ìš”.")
    input_text = input(">> ")
    result = predict_traits(input_text)
    print("\nğŸ“Š ì˜ˆì¸¡ëœ ì„±í–¥ ì ìˆ˜:")
    for trait, score in result.items():
        print(f"{trait}: {score}")